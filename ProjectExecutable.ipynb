{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Padinant/CodeFest-2025-Garbage-Classifier-Project/blob/main/ProjectExecutable.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Team Info\n",
        "Team # 10: Padina Nasiri Toussi, Timothy Winans\n",
        "\n",
        "This colab notebook is the executable trains the Computer Vision model used for our team's Spring 2025 CodeFest project based on challenge #1\n",
        "\n",
        "It aims to correctly classify garbage images into 10 categories for the purpose of recycling\n",
        "\n",
        "For our final model, we have used the pretrained MobileNet model -known for its efficiency and low-cost performance- as base model, which we finetuned on our data.\n",
        "\n",
        "The model has been trained on the dataset found here:\n",
        "https://www.kaggle.com/datasets/sumn2u/garbage-classification-v2?resource=download\n",
        "\n",
        "We have named our project GAIA, after the ancient Greek goddess of nature and all life - which we believe is an apt name as our project aims to give recyclable waste a chance of a new life.\n",
        "\n",
        "Our model is called Oscar, The G.A.I.A (Green Automated Intelligent Assistant) Garbage Detector - Which we think is a whimsical name, while still being on theme and carrying our project name as an acronym.\n"
      ],
      "metadata": {
        "id": "GK-bcP00qzYV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Why is this Eco-Friendly?\n",
        "- Efficiently trained model, trained on a few epochs\n",
        "- An already energy efficient and low cost base model (MobileNet)\n",
        "- Could be deployed on low-power devices like Raspberry Pi or mobile phones\n",
        "- The dataset is processed efficiently, using image compression and resizing techniques to limit storage and computational overhead.\n",
        "- Batch processing and on-the-fly augmentation reduce the need for high memory and storage requirements.\n",
        "- We aim to minimize contaminated and missed recyclings by focusing on our model's percision and recall rates, alongside its accuracy"
      ],
      "metadata": {
        "id": "IOBSH4XOrkHw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Info\n",
        "The Executable Google Colab Notebook for Team 10 - Spring 2025 CodeFest\n",
        "\n",
        "Members:\n",
        "- Padina Nasiri Toussi\n",
        "- Timothy Winans"
      ],
      "metadata": {
        "id": "s_kJIxSUlZ5I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instructions for Execution\n",
        "This section covers instructions on how to use this Google Colab file.\n",
        "\n",
        "In this notebook, you can import our fine-tuned computer vision CNN model, and utilize it in producing predictions for the recycling category of input images.\n",
        "\n",
        "You can import the model in the 'Importing the Model' section.\n",
        "\n",
        "2 main functions have been provided in 'Main Functions' which allow you to use the model to predict the recycling category of 1 or multiple images.\n",
        "\n",
        "There are also some helper function in Useful Helper Functions which you could also use to help preprocess your input data, or even download our original dataset from kaggle"
      ],
      "metadata": {
        "id": "AErIQHLulqz-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to Run our Garbage Detector Model on this Google Colab\n",
        "\n",
        "This guide provides a step-by-step approach to running our fine-tuned MobileNet Garbage Detector notebook on Google Colab.\n",
        "\n",
        "## Step 0: Have the project's shared Google Drive folder called Final_Submission handy\n",
        "This is where our final trained models can be found\n",
        "\n",
        "## Step 1: Run the cell under 'Importing Some Useful Libraries'\n",
        "This imports libraries used throughout the notebook\n",
        "\n",
        "## Step 2: Run the cells under 'Useful Constants and Variables'\n",
        "A few variables used throught the notebook\n",
        "\n",
        "## Step 3: Import model from the Final_Submission folder to Google Colab\n",
        "\n",
        "Option 1: Open from Local Machine\n",
        "\n",
        "Option 2: Open Model from Google Drive (Recommended)\n",
        "\n",
        "for more instructions on these options see 'Importing the Model'\n",
        "\n",
        "## Step 4: Run the cells under 'Useful Helper Functions'\n",
        "These are some helper functions, but they may be also of use to you in preparing input data for the model\n",
        "\n",
        "## Step 5: Run the cells under 'Main Functions'\n",
        "You can call these functions directly to get our model's predictions on input images, in the next step.\n",
        "\n",
        "## Step 6: Testing\n",
        "In 'Test Section', feel free to write any code to evaluate and play with our model"
      ],
      "metadata": {
        "id": "A_a4X6ehYaL8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Some Useful Libraries"
      ],
      "metadata": {
        "id": "w1bu8jBuoaaW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "import os\n",
        "import cv2\n",
        "import imghdr\n",
        "import glob\n",
        "from itertools import islice\n",
        "\n",
        "# ensure any matplotlib graphs will be displayed inline\n",
        "%matplotlib inline\n",
        "\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "PNBrwwIgoZ_x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a29cc47-4fa3-47d8-8868-9e2b03fc162c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-3ecb90bb14c8>:9: DeprecationWarning: 'imghdr' is deprecated and slated for removal in Python 3.13\n",
            "  import imghdr\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Useful Constants and Variables\n",
        "These variables will later help in building our main functions"
      ],
      "metadata": {
        "id": "b9u2D-SBwU30"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define global variable - CATEGORY_NAMES (array of len 10)\n",
        "CATEGORY_NAMES = ['battery', 'biological', 'cardboard', 'clothes', 'glass', 'metal', 'paper', 'plastic', 'shoes', 'trash']"
      ],
      "metadata": {
        "id": "LmwHbiobtGRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# resizing variables - these will help in converting images into model input\n",
        "img_width = 250\n",
        "img_height = 250\n",
        "img_size = (img_width, img_height)"
      ],
      "metadata": {
        "id": "eYFusIZDlsN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing the Model\n",
        "There are 2 options to import the model: offline through upload to the colabatory, and online through mounting Google Drive\n",
        "\n",
        "The online option requires you to link your google Drive account to this colabatory. If you prefer not to do that, please follow the instructions for the offline option.\n",
        "\n"
      ],
      "metadata": {
        "id": "L3S_T9E6l727"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Option 1: Offline Model Import\n",
        "Instructions:\n",
        "\n",
        "Navigate to the shared folder submitted by the team (named: Final_Submission). From there, download the keras format of the model (titled: MobileNetGarbageDetector.keras) into your local computer.\n",
        "\n",
        "Then, navigate to the files section of this colabatory (on the left side of the screen), and 'press the upload to session storage button'. From there, you can upload our keras model into the session storage files.\n",
        "\n",
        "After that, run the code below."
      ],
      "metadata": {
        "id": "-BJBE6ixmnvN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OkdCZ0dLlTQ3"
      },
      "outputs": [],
      "source": [
        "# importing the model from the notebook session storage\n",
        "base_path = '/content/'\n",
        "model_name = 'MobileNetGarbageDetector1_9.keras'\n",
        "retrieval_path = base_path + model_name\n",
        "\n",
        "model = load_model(retrieval_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Option 2: Online Model Import"
      ],
      "metadata": {
        "id": "REKYir9QmpDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True) # this might prompt a pop-up from Google!\n",
        "\n",
        "# add your retrieval path here\n",
        "base_path = '/content/drive/'\n",
        "model_name = 'MobileNetGarbageDetector1_9.keras'\n",
        "retrieval_path = base_path + model_name\n",
        "\n",
        "model = load_model(retrieval_path)"
      ],
      "metadata": {
        "id": "D20de5pkmxZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Useful Helper Functions\n",
        "These functions will later help in building our main functions, or can be useful to the user when preparing data to evaluate our model with"
      ],
      "metadata": {
        "id": "zMwMozP3sTGZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Some useful kaggle-related functions\n",
        "\n",
        "def install_kaggle():\n",
        "  # installs the kaggle API client in the notebook\n",
        "  !pip install kaggle\n",
        "\n",
        "\n",
        "def download_kaggle_dataset(kaggle_username: str, kaggle_userkey: str):\n",
        "  \"\"\" Takes in the user's kaggle information as input, and returns the downloaded\n",
        "  garbage dataset which was used for this challenge.\n",
        "  The dataset will be downloaded from:\n",
        "    https://www.kaggle.com/datasets/sumn2u/garbage-classification-v2?resource=download\n",
        "\n",
        "  The function unzips the dataset and loads it into the notebook's session storage\n",
        "  \"\"\"\n",
        "\n",
        "  os.environ['KAGGLE_USERNAME'] = kaggle_username  # kaggle username\n",
        "  os.environ['KAGGLE_KEY'] = kaggle_userkey # kaggle key\n",
        "\n",
        "  # Download the garbage dataset\n",
        "  !kaggle datasets download -d sumn2u/garbage-classification-v2\n",
        "\n",
        "  # Unzip the downloaded dataset\n",
        "  !unzip garbage-classification-v2.zip"
      ],
      "metadata": {
        "id": "PNCGGMXGv9Tl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Functions relating to image quirks\n",
        "\n",
        "def drop_unexpected_images(base_path = '/content/garbage-dataset/', folder_names = CATEGORY_NAMES):\n",
        "  # Removing any unexpected files (anything not JPEG, PNG, JPG)\n",
        "  # Prerequisite: The test images have been stored in inner folders, whose names match folder_names\n",
        "  # Prerequisite: Base path is the path to an outer folder, storing 1 or more inner category folders containing images\n",
        "  # this function can be useful as there are a few images in the original dataset which might need to be cleaned\n",
        "\n",
        "  count = 0\n",
        "  for folder_name in folder_names:\n",
        "    folder_path = base_path + folder_name\n",
        "\n",
        "    all_files = glob.glob(os.path.join(folder_path, \"*\"))\n",
        "\n",
        "    for file in all_files:\n",
        "      file_type = imghdr.what(file)  # Detect file format\n",
        "      if file_type not in [\"jpeg\", \"png\", \"jpg\"]:\n",
        "          print(f\"Unknown image type: {file} (Detected as {file_type})\")\n",
        "\n",
        "          # Delete file from the dataset\n",
        "          os.remove(file)\n",
        "          print(f\"{file} was successfully removed from dataset\")\n",
        "          count += 1\n",
        "\n",
        "  print(f\"{count} images have been removed from dataset.\")\n",
        "\n",
        "\n",
        "# Several useful functions to remvoe transparency from an image, or image path(s)\n",
        "def remove_transparency_from_image(img, bg_color=(255, 255, 255)):\n",
        "  \"\"\"\n",
        "  This function removes the alpha channel (quantifying opaqueness/transparency) of an image, if the channel exists.\n",
        "  It is given the actual image, and the background color as input, and returns the updated image\n",
        "  bg_color is the RGB background color the value of the alpha channel will be replaced and blended with\n",
        "  By default, bg_color has been set to white, but you can also set it to black, or any other color\n",
        "  Rewrites the image to only have the 3 RGB channels\n",
        "  \"\"\"\n",
        "  if img.shape[-1] == 4:  # Has an alpha channel\n",
        "    alpha = img[:, :, 3] / 255.0  # Normalize alpha to 0-1\n",
        "    img_rgb = img[:, :, :3]  # Get RGB channels\n",
        "\n",
        "    # Blend with white background --> ie: opaque pixels are unaffected, transparent pixels become white\n",
        "    img_new = (1. - alpha[:, :, None]) * np.array(bg_color, dtype=np.float32) + alpha[:, :, None] * img_rgb\n",
        "    img_new = img_new.astype(np.uint8)\n",
        "\n",
        "    return img_new  # return updated image\n",
        "\n",
        "  return img\n",
        "\n",
        "def remove_transparency(image_path, bg_color=(255, 255, 255)):\n",
        "  \"\"\"\n",
        "  This function removes the alpha channel (quantifying opaqueness/transparency) of an image if the channel exists.\n",
        "  It is given the image path and background color as input, and doesn't return anything\n",
        "  bg_color is the RGB background color the value of the alpha channel will be replaced and blended with\n",
        "  By default, bg_color has been set to white, but you can also set it to black, or any other color\n",
        "  Rewrites the image to only have the 3 RGB channels\n",
        "  \"\"\"\n",
        "  img = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)  # Load image with all channels (including alpha)\n",
        "  if img.shape[-1] == 4:  # Has an alpha channel\n",
        "    alpha = img[:, :, 3] / 255.0  # Normalize alpha to 0-1\n",
        "    img_rgb = img[:, :, :3]  # Get RGB channels\n",
        "\n",
        "    # Blend with white background --> ie: opaque pixels are unaffected, transparent pixels become white\n",
        "    img_new = (1. - alpha[:, :, None]) * np.array(bg_color, dtype=np.float32) + alpha[:, :, None] * img_rgb\n",
        "    img_new = img_new.astype(np.uint8)\n",
        "\n",
        "    cv2.imwrite(image_path, img_new)  # Overwrite the image\n",
        "\n",
        "def remove_transparency_from_folders(base_path = '/content/garbage-dataset/', folder_names = CATEGORY_NAMES, target_image_type = \"jpg\"):\n",
        "  \"\"\"\n",
        "  This function applies remove_transparency to all images of a given type inside the session storage\n",
        "\n",
        "  Prerequisites:\n",
        "  # Base path is the path to an outer folder, storing 1 or more inner category folders containing images\n",
        "  # The test images have been stored in inner folders, whose names match folder_names\n",
        "  # target_image_type has to be in [\"jpeg\", \"png\", \"jpg\"]\n",
        "  \"\"\"\n",
        "  for folder_name in folder_names:\n",
        "    folder_path = base_path + folder_name\n",
        "\n",
        "    image_paths = glob.glob(os.path.join(folder_path, \"*.\" + target_image_type))  # Find all instances of the target type\n",
        "    print(f\"Processing {len(image_paths)} images in {folder_path}...\")\n",
        "\n",
        "    # applying remove_transparency to each file\n",
        "    for file in image_paths:\n",
        "      remove_transparency(file)\n",
        "\n",
        "  print(f\"All transparent images of type {target_image_type} have been processed.\")"
      ],
      "metadata": {
        "id": "NTrnSnhKcoNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Functions\n",
        "These functions can be called by the user to provide functionality, and test our model"
      ],
      "metadata": {
        "id": "4ZD5YGisv89A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def image_to_category(img, output_type = 'single_category'):\n",
        "  \"\"\"\n",
        "  # Takes in an image as input, and returns the predicted category\n",
        "  # Precondition: the image type (as determined by imghdr.what()) must be one of JPG, JPEG, PNG\n",
        "  # Precondition: the variable output_type may be one of the following options :\n",
        "    # 'single_category': returns the top predicted category, and the corresponding predicted percentage\n",
        "    # 'top_three': returns the top three predicted categories in order of likelihood, and the corresponding percentage\n",
        "    # 'all_categories': returns the predicted percentage for all 10 possible categories\n",
        "  # Postcondition: the output is a dictionary of type {string: float}.\n",
        "  # The dictionary will store {category_name: prediction_value} and be ordered in order of how high the predicted value was\n",
        "\n",
        "  # Function Explanation\n",
        "  # step 0: A precondition is that the image has to be of an acceptable file format (PNG, JPG, JPEG)\n",
        "  # step 1: remove transparency (alpha channel) from image\n",
        "  # step 2: convert to correct input format\n",
        "  # step 3: get model output\n",
        "  # step 4: use the results to create a dictionary\n",
        "  # step 5: return dictionary\n",
        "  \"\"\"\n",
        "\n",
        "  # A bit of setup\n",
        "  if output_type not in ('single_category', 'top_three', 'all_categories'):\n",
        "    output_type = 'single_category'\n",
        "  image = img.copy  # to avoid modifying the original image\n",
        "\n",
        "  # step 1: remove transparency (alpha channel) from image\n",
        "  remove_transparency_from_image(image)\n",
        "\n",
        "  # step 2: convert to correct input format (tf.data.Dataset)\n",
        "  image = tf.image.resize(image, img_size)\n",
        "\n",
        "  image = tf.expand_dims(image, axis=0) # Add a new batch dimension to the front of the image\n",
        "                                        # image is now of shape: [1, img_width, img_height, 3]\n",
        "  input_dataset = tf.data.Dataset.from_tensor_slices(image)\n",
        "\n",
        "  # step 3: get model output\n",
        "  predictions = model.predict(input_dataset) # prediction is an array of len 10 corresponding to the different categories\n",
        "\n",
        "  # step 4a: use the results to create a dictionary corresponding to categories\n",
        "  classifications = {}\n",
        "  for i in range(0, 10):\n",
        "    classifications[CATEGORY_NAMES[i]] = predictions[i] # {category_name: predicted_value}\n",
        "\n",
        "  # step 4b: reorder the dictionary based on how high the prediction for each category was\n",
        "  sorted_classifications = dict(sorted(classifications.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "  # step 5: return (the appropriate version) of the dictionary\n",
        "  if output_type == 'single_category':\n",
        "    return dict(islice(sorted_classifications.items(), 1))  # Using islice to get the first key value pair of the dict\n",
        "  elif output_type == 'top_three':\n",
        "    return dict(islice(sorted_classifications.items(), 3))  # Using islice to get the first 3 key value pairs of the dict\n",
        "  else:\n",
        "    return sorted_classifications # no need to change anything\n"
      ],
      "metadata": {
        "id": "WGgL__XrsYTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def image_folder_to_category(folder_path, output_type = 'single_category'):\n",
        "  \"\"\"\n",
        "  # Takes in a folder path as input and returns a list of predicted categories\n",
        "  # Precondition: folder_path has to be the path to a folder in session storage, containing image files\n",
        "  # Precondition: The type of those images (as determined by imghdr.what()) must be in (JPG, JPEG, PNG)\n",
        "  # Precondition: the variable output_type may be one of the following options:\n",
        "    # 'single_category': Each dictionary includes the top predicted category, and the corresponding predicted percentage, for corresponding image\n",
        "    # 'top_three': Each dictionary includes the top three predicted categories in order of likelihood, and the corresponding percentage, for corresponding image\n",
        "    # 'all_categories': Each dictionary includes the predicted percentage for all 10 possible categories, for corresponding image\n",
        "  # Postcondition: the output is a list of dictionaries of type {string: float}.\n",
        "  # The order of the elements in the list, will correspond to that of the images in the folder\n",
        "  # Each inner dictionary will store {category_name: prediction_value}\n",
        "  # Each inner dictionary will be internally ordered in in terms of size of prediction value for each category\n",
        "\n",
        "  # Function Explanation\n",
        "  # step 0: drop unexpected files\n",
        "  # step 1: remove transparency (alpha channel) from images\n",
        "  # step 2: convert to correct input format\n",
        "  # step 3: get model output\n",
        "  # step 4: use the results to create the final list\n",
        "  # step 5: return list\n",
        "  \"\"\"\n",
        "  # A bit of setup\n",
        "  if output_type not in ('single_category', 'top_three', 'all_categories'):\n",
        "    output_type = 'single_category'\n",
        "\n",
        "  # step 0: drop unexpected file formats\n",
        "  drop_count = 0\n",
        "  all_files = glob.glob(os.path.join(folder_path, \"*\"))\n",
        "\n",
        "  for file in all_files:\n",
        "    file_type = imghdr.what(file)  # Detect file format\n",
        "    if file_type not in [\"jpeg\", \"png\", \"jpg\"]:\n",
        "        print(f\"Unknown image type: {file} (Detected as {file_type})\")\n",
        "\n",
        "        # Delete file from the dataset\n",
        "        os.remove(file)\n",
        "        print(f\"{file} was successfully removed from dataset\")\n",
        "        drop_count += 1\n",
        "\n",
        "  print(f\"{drop_count} images have been removed from dataset.\")\n",
        "\n",
        "  # step 1: Remove transparency\n",
        "  for target_image_type in (\"jpeg\", \"png\", \"jpg\"):\n",
        "    image_paths = glob.glob(os.path.join(folder_path, \"*.\" + target_image_type))  # Find all instances of the target type\n",
        "\n",
        "    print(f\"Processing {len(image_paths)} images of type {target_image_type} in {folder_path}...\")\n",
        "\n",
        "    # applying remove_transparency to each file\n",
        "    for file in image_paths:\n",
        "      remove_transparency(file)\n",
        "\n",
        "  print(f\"All transparent images of type {target_image_type} have been processed.\")\n",
        "\n",
        "\n",
        "  # step 2: convert to correct input format (tf.data.Dataset)\n",
        "  input_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "    directory=folder_path,\n",
        "    image_size=img_size,  # Resize images\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        "    seed=25,  # Seed is not needed, since we're not shuffling\n",
        "    )\n",
        "\n",
        "  # step 3: get model output\n",
        "  folder_predictions = model.predict(input_dataset) # prediction is an array of len 10 corresponding to the different categories\n",
        "\n",
        "  # step 4: create the return list\n",
        "  result = []\n",
        "  for predictions in folder_predictions:\n",
        "    # step 4a: use the results to create a dictionary corresponding to categories\n",
        "    classifications = {}\n",
        "    for i in range(0, 10):\n",
        "      classifications[CATEGORY_NAMES[i]] = predictions[i] # {category_name: predicted_value}\n",
        "\n",
        "    # step 4b: reorder the dictionary based on how high the prediction for each category was\n",
        "    sorted_classifications = dict(sorted(classifications.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "    # step 5: return (the appropriate version) of the list\n",
        "    if output_type == 'single_category':\n",
        "      result.append(dict(islice(sorted_classifications.items(), 1)))  # Using islice to get the first key value pair of the dict\n",
        "    elif output_type == 'top_three':\n",
        "      result.append(dict(islice(sorted_classifications.items(), 3)))  # Using islice to get the first 3 key value pairs of the dict\n",
        "    else:\n",
        "      result.append(sorted_classifications) # no need to change anything\n",
        "\n",
        "  return result"
      ],
      "metadata": {
        "id": "lfLRvq86hg4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Section\n",
        "\n",
        "Here, you can experiment with the provided functions (and even the dataset), to evaluate our model\n"
      ],
      "metadata": {
        "id": "0wreXc5fdKLw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feel free to add more code!"
      ],
      "metadata": {
        "id": "noBkCKgeoWBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Thank You\n",
        "\n",
        "Thank you for viewing our code.\n",
        "\n",
        "Have a nice day!\n",
        "\n",
        "This project is open-source under the MIT License."
      ],
      "metadata": {
        "id": "egmDMU4tr5ud"
      }
    }
  ]
}